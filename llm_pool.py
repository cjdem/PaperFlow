"""
LLM æ± ç®¡ç†å™¨ - ä»æ•°æ®åº“è¯»å–é…ç½®ï¼Œæ”¯æŒåŠ¨æ€åˆ·æ–°
"""
from openai import AsyncOpenAI
from llm_service import get_enabled_providers, import_from_json


class LLMManager:
    def __init__(self):
        # é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œå°è¯•ä» JSON å¯¼å…¥é…ç½®
        imported = import_from_json()
        if imported > 0:
            print(f"âœ… å·²ä» llm_config.json å¯¼å…¥ {imported} ä¸ªæä¾›å•†é…ç½®")
        
        # æ„å»ºæ± å­
        self.pools = {"metadata": [], "analysis": []}
        self.reload_config()

    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®ï¼ˆä»æ•°æ®åº“ï¼‰"""
        self.pools = {
            "metadata": self._build_pool("metadata"),
            "analysis": self._build_pool("analysis"),
        }
        print("=" * 40)
        print("ğŸ”Œ LLM é…ç½®å·²åŠ è½½")
        print(f"   - Metadata ä¸»åŠ›: {self._get_first_name('metadata')}")
        print(f"   - Analysis ä¸»åŠ›: {self._get_first_name('analysis')}")
        print("=" * 40)

    def _get_first_name(self, pool_name: str) -> str:
        pool = self.pools.get(pool_name, [])
        if not pool:
            return "æ— å¯ç”¨é…ç½®"
        node = pool[0]
        return f"[{node['model']}] @ {node['provider']}"

    def _build_pool(self, pool_type: str) -> list:
        """ä»æ•°æ®åº“æ„å»ºå®¢æˆ·ç«¯æ± """
        providers = get_enabled_providers(pool_type)
        client_pool = []

        for entry in providers:
            base_url = entry.get("base_url", "").strip()
            keys = [k.strip() for k in entry.get("api_key", "").split(",") if k.strip()]
            models = [m.strip() for m in entry.get("models", "").split(",") if m.strip()]

            for key in keys:
                client = AsyncOpenAI(api_key=key, base_url=base_url)
                for model in models:
                    provider = base_url.split("//")[-1].split("/")[0]
                    client_pool.append({
                        "client": client,
                        "model": model,
                        "provider": provider,
                        "is_primary": entry.get("is_primary", False),
                        "id": f"[{model}] @ {provider}",
                    })

        return client_pool

    async def chat(self, pool_name: str, messages: list, response_format=None, 
                   temperature: float = 0.7, validator=None):
        """è°ƒç”¨ LLMï¼Œä¸»å¤‡æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢"""
        target_pool = self.pools.get(pool_name, [])
        if not target_pool:
            raise ValueError(f"âŒ æ± å­ {pool_name} ä¸ºç©ºï¼Œè¯·åœ¨ç®¡ç†é¢æ¿é…ç½® LLM æä¾›å•†")

        last_error = None

        for i, node in enumerate(target_pool):
            try:
                if i > 0:
                    print(f"   âš ï¸ [ä¸»åŠ›æŒ‚äº†] åˆ‡æ¢å¤‡ç”¨çº¿è·¯ {i}: {node['id']}")

                kwargs = {
                    "model": node["model"],
                    "messages": messages,
                    "temperature": temperature,
                }
                if response_format:
                    kwargs["response_format"] = response_format

                response = await node["client"].chat.completions.create(**kwargs)

                # è´¨æ£€ç¯èŠ‚
                if validator:
                    content = response.choices[0].message.content
                    if not validator(content):
                        raise ValueError(f"å†…å®¹è´¨æ£€æœªé€šè¿‡: {content[:50]}...")

                return response

            except Exception as e:
                print(f"   âš ï¸ é€šé“æ— æ•ˆ [{node['id']}]: {e}")
                last_error = e
                continue

        raise last_error


# å…¨å±€å®ä¾‹
llm_manager = LLMManager()
