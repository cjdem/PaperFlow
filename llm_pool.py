"""
LLM æ± ç®¡ç†å™¨ - ä»æ•°æ®åº“è¯»å–é…ç½®ï¼Œæ”¯æŒåŠ¨æ€åˆ·æ–°
æ”¯æŒ:
  - æŒ‰æƒé‡è´Ÿè½½å‡è¡¡
  - æ™ºèƒ½é‡è¯•ï¼ˆæ’é™¤å¤±è´¥çš„é€šé“ï¼‰
  - OpenAI å’Œ Gemini API æ ¼å¼ï¼ˆå«è‡ªå®šä¹‰åœ°å€ï¼‰
  - æµå¼å“åº”æ”¯æŒ
"""
import random
import asyncio
import httpx
from openai import AsyncOpenAI
from llm_service import get_enabled_providers, import_from_json
from db_service import get_config
from log_service import llm_logger, get_logger

logger = get_logger("llm_pool")


class GeminiClientWrapper:
    """
    Gemini API å®¢æˆ·ç«¯åŒ…è£…å™¨
    æ”¯æŒè‡ªå®šä¹‰ base_urlï¼ˆå¦‚ä¸­è½¬æœåŠ¡ï¼‰
    """
    def __init__(self, api_key: str, base_url: str = None):
        self.api_key = api_key
        # é»˜è®¤ä½¿ç”¨ Google å®˜æ–¹ APIï¼Œä¹Ÿæ”¯æŒè‡ªå®šä¹‰åœ°å€
        self.base_url = base_url.rstrip("/") if base_url else "https://generativelanguage.googleapis.com/v1beta"
    
    async def create_chat_completion(self, model: str, messages: list, temperature: float = 0.7, **kwargs):
        """è°ƒç”¨ Gemini API å¹¶è¿”å› OpenAI å…¼å®¹çš„å“åº”æ ¼å¼"""
        # å°† OpenAI æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º Gemini æ ¼å¼
        gemini_contents = []
        system_instruction = None
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                system_instruction = content
            elif role == "assistant":
                gemini_contents.append({"role": "model", "parts": [{"text": content}]})
            else:  # user
                gemini_contents.append({"role": "user", "parts": [{"text": content}]})
        
        # æ„å»ºè¯·æ±‚ä½“
        request_body = {
            "contents": gemini_contents,
            "generationConfig": {
                "temperature": temperature
            }
        }
        
        if system_instruction:
            request_body["systemInstruction"] = {"parts": [{"text": system_instruction}]}
        
        # æ„å»º URL
        url = f"{self.base_url}/models/{model}:generateContent?key={self.api_key}"
        
        # å‘é€è¯·æ±‚ - ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¶…æ—¶é…ç½®
        timeout = httpx.Timeout(300.0, connect=30.0)  # æ€»è¶…æ—¶5åˆ†é’Ÿï¼Œè¿æ¥è¶…æ—¶30ç§’
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(url, json=request_body)
            response.raise_for_status()
            data = response.json()
        
        # åŒ…è£…æˆ OpenAI å…¼å®¹çš„å“åº”æ ¼å¼
        return GeminiResponseWrapper(data)


class GeminiResponseWrapper:
    """å°† Gemini å“åº”åŒ…è£…æˆ OpenAI å…¼å®¹æ ¼å¼"""
    def __init__(self, gemini_response: dict):
        self.choices = [GeminiChoiceWrapper(gemini_response)]


class GeminiChoiceWrapper:
    """Gemini choice åŒ…è£…å™¨"""
    def __init__(self, gemini_response: dict):
        self.message = GeminiMessageWrapper(gemini_response)


class GeminiMessageWrapper:
    """Gemini message åŒ…è£…å™¨"""
    def __init__(self, gemini_response: dict):
        try:
            candidates = gemini_response.get("candidates", [])
            if candidates:
                parts = candidates[0].get("content", {}).get("parts", [])
                if parts:
                    self.content = parts[0].get("text", "")
                else:
                    self.content = ""
            else:
                self.content = ""
        except Exception:
            self.content = ""


class AnthropicClientWrapper:
    """
    Anthropic API å®¢æˆ·ç«¯åŒ…è£…å™¨
    æ”¯æŒè‡ªå®šä¹‰ base_urlï¼ˆå¦‚ä¸­è½¬æœåŠ¡ï¼‰
    """
    def __init__(self, api_key: str, base_url: str = None):
        self.api_key = api_key
        self.base_url = base_url.rstrip("/") if base_url else "https://api.anthropic.com"
    
    async def create_chat_completion(self, model: str, messages: list, temperature: float = 0.7, **kwargs):
        """è°ƒç”¨ Anthropic API å¹¶è¿”å› OpenAI å…¼å®¹çš„å“åº”æ ¼å¼"""
        # å°† OpenAI æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º Anthropic æ ¼å¼
        anthropic_messages = []
        system_content = None
        
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                system_content = content
            else:
                anthropic_messages.append({"role": role, "content": content})
        
        # æ„å»ºè¯·æ±‚ä½“
        request_body = {
            "model": model,
            "max_tokens": kwargs.get("max_tokens", 8192),
            "messages": anthropic_messages,
            "temperature": temperature
        }
        
        if system_content:
            request_body["system"] = system_content
        
        # æ„å»º URL
        url = f"{self.base_url}/v1/messages"
        
        # å‘é€è¯·æ±‚ - ä½¿ç”¨æ›´ç»†ç²’åº¦çš„è¶…æ—¶é…ç½®
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        timeout = httpx.Timeout(300.0, connect=30.0)  # æ€»è¶…æ—¶5åˆ†é’Ÿï¼Œè¿æ¥è¶…æ—¶30ç§’
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(url, json=request_body, headers=headers)
            response.raise_for_status()
            data = response.json()
        
        # åŒ…è£…æˆ OpenAI å…¼å®¹çš„å“åº”æ ¼å¼
        return AnthropicResponseWrapper(data)


class AnthropicResponseWrapper:
    """å°† Anthropic å“åº”åŒ…è£…æˆ OpenAI å…¼å®¹æ ¼å¼"""
    def __init__(self, anthropic_response: dict):
        self.choices = [AnthropicChoiceWrapper(anthropic_response)]


class AnthropicChoiceWrapper:
    """Anthropic choice åŒ…è£…å™¨"""
    def __init__(self, anthropic_response: dict):
        self.message = AnthropicMessageWrapper(anthropic_response)


class AnthropicMessageWrapper:
    """Anthropic message åŒ…è£…å™¨"""
    def __init__(self, anthropic_response: dict):
        try:
            content_blocks = anthropic_response.get("content", [])
            if content_blocks:
                # Anthropic è¿”å› content æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ª text ç±»å‹çš„å†…å®¹
                for block in content_blocks:
                    if block.get("type") == "text":
                        self.content = block.get("text", "")
                        return
            self.content = ""
        except Exception:
            self.content = ""


class LLMManager:
    def __init__(self):
        # é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œå°è¯•ä» JSON å¯¼å…¥é…ç½®
        imported = import_from_json()
        if imported > 0:
            logger.info(f"âœ… å·²ä» llm_config.json å¯¼å…¥ {imported} ä¸ªæä¾›å•†é…ç½®")
        
        # æ„å»ºæ± å­
        self.pools = {"metadata": [], "analysis": []}
        self.reload_config()

    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®ï¼ˆä»æ•°æ®åº“ï¼‰"""
        self.pools = {
            "metadata": self._build_pool("metadata"),
            "analysis": self._build_pool("analysis"),
        }
        logger.info("ğŸ”Œ LLM é…ç½®å·²åŠ è½½")
        logger.info(f"   - Metadata ä¸»åŠ›: {self._get_first_name('metadata')}")
        logger.info(f"   - Analysis ä¸»åŠ›: {self._get_first_name('analysis')}")

    def _get_first_name(self, pool_name: str) -> str:
        """è·å–ä¸»åŠ›æ¨¡å‹åç§°ï¼ˆis_primary ä¼˜å…ˆï¼Œç„¶åæŒ‰ priority æ’åºçš„ç¬¬ä¸€ä¸ªï¼‰"""
        pool = self.pools.get(pool_name, [])
        if not pool:
            return "æ— å¯ç”¨é…ç½®"
        node = pool[0]
        primary_tag = " âœ“ä¸»" if node.get('is_primary') else ""
        return f"[{node['model']}] @ {node['provider']} ({node['api_type']}){primary_tag}"

    def _build_pool(self, pool_type: str) -> list:
        """ä»æ•°æ®åº“æ„å»ºå®¢æˆ·ç«¯æ± """
        providers = get_enabled_providers(pool_type)
        client_pool = []

        for entry in providers:
            base_url = entry.get("base_url", "").strip()
            keys = [k.strip() for k in entry.get("api_key", "").split(",") if k.strip()]
            models = [m.strip() for m in entry.get("models", "").split(",") if m.strip()]
            weight = entry.get("weight", 10)
            api_type = entry.get("api_type", "openai")

            for key in keys:
                # æ ¹æ® api_type åˆ›å»ºä¸åŒçš„å®¢æˆ·ç«¯
                if api_type == "gemini":
                    client = GeminiClientWrapper(api_key=key, base_url=base_url or None)
                elif api_type == "anthropic":
                    client = AnthropicClientWrapper(api_key=key, base_url=base_url or None)
                else:  # é»˜è®¤ openai - æ·»åŠ è¶…æ—¶é…ç½®
                    client = AsyncOpenAI(
                        api_key=key,
                        base_url=base_url,
                        timeout=httpx.Timeout(300.0, connect=30.0)  # æ€»è¶…æ—¶5åˆ†é’Ÿï¼Œè¿æ¥è¶…æ—¶30ç§’
                    )
                
                for model_index, model in enumerate(models):
                    provider = base_url.split("//")[-1].split("/")[0] if base_url else "googleapis.com"
                    client_pool.append({
                        "client": client,
                        "model": model,
                        "provider": provider,
                        "api_type": api_type,
                        "is_primary": entry.get("is_primary", False),
                        "weight": weight,
                        "priority": entry.get("priority", 1),
                        "model_index": model_index,  # æ¨¡å‹åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ï¼Œç”¨äºä¿è¯åŒä¸€æä¾›å•†å†…çš„æ¨¡å‹æŒ‰é¡ºåºè°ƒç”¨
                        "id": f"[{model}] @ {provider}",
                    })

        # æŒ‰ is_primary DESC, priority ASC, model_index ASC æ’åº
        # ç¡®ä¿ï¼š1. ä¸»æ¨¡å‹ä¼˜å…ˆ 2. ä¼˜å…ˆçº§é«˜çš„å…ˆè°ƒç”¨ 3. åŒä¸€æä¾›å•†å†…æŒ‰é€—å·é¡ºåºè°ƒç”¨
        client_pool.sort(key=lambda x: (
            not x.get("is_primary", False),  # is_primary=True æ’å‰é¢
            x.get("priority", 1),             # priority å°çš„æ’å‰é¢
            x.get("model_index", 0)           # model_index å°çš„æ’å‰é¢
        ))
        
        return client_pool

    def _select_provider(self, pool: list, exclude_ids: set) -> dict:
        """æŒ‰æƒé‡éšæœºé€‰æ‹©ä¸€ä¸ªæä¾›å•†ï¼ˆæ’é™¤å·²å¤±è´¥çš„ï¼‰"""
        eligible = [p for p in pool if p["id"] not in exclude_ids]
        if not eligible:
            return None
        
        weights = [p["weight"] for p in eligible]
        selected = random.choices(eligible, weights=weights, k=1)[0]
        return selected

    def _get_max_retries(self) -> int:
        """è·å–æœ€å¤§é‡è¯•æ¬¡æ•°é…ç½®"""
        try:
            val = get_config("llm_max_retries", "3")
            return int(val)
        except (ValueError, TypeError):
            return 3

    async def chat(self, pool_name: str, messages: list, response_format=None, 
                   temperature: float = 0.7, validator=None):
        """
        è°ƒç”¨ LLMï¼Œæ”¯æŒ:
          - é¡ºåºæ•…éšœè½¬ç§» (æŒ‰ priority é¡ºåº)
          - å•ç‚¹ç«­å°½é‡è¯• (æ¯ä¸ª Provider é‡è¯• N æ¬¡åå†åˆ‡æ¢)
          - OpenAIã€Gemini å’Œ Anthropic API
        """
        target_pool = self.pools.get(pool_name, [])
        if not target_pool:
            raise ValueError(f"âŒ æ± å­ {pool_name} ä¸ºç©ºï¼Œè¯·åœ¨ç®¡ç†é¢æ¿é…ç½® LLM æä¾›å•†")

        max_retries = self._get_max_retries()
        last_error = None

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºéå†æ¯ä¸ª Provider
        for node in target_pool:
            provider_id = node['id']
            
            # å¯¹å½“å‰ Provider è¿›è¡Œæœ€å¤š max_retries æ¬¡å°è¯•
            for attempt in range(max_retries):
                try:
                    if attempt == 0:
                        llm_logger.log_request(pool_name, provider_id, node['api_type'], node.get('priority', 1))
                    else:
                        llm_logger.log_retry(attempt, max_retries, provider_id, str(last_error))

                    # æ ¹æ® API ç±»å‹è°ƒç”¨ä¸åŒçš„æ–¹æ³•
                    if node["api_type"] == "gemini":
                        response = await node["client"].create_chat_completion(
                            model=node["model"],
                            messages=messages,
                            temperature=temperature
                        )
                    elif node["api_type"] == "anthropic":
                        response = await node["client"].create_chat_completion(
                            model=node["model"],
                            messages=messages,
                            temperature=temperature
                        )
                    else:  # OpenAI
                        kwargs = {
                            "model": node["model"],
                            "messages": messages,
                            "temperature": temperature,
                        }
                        if response_format:
                            kwargs["response_format"] = response_format
                        response = await node["client"].chat.completions.create(**kwargs)

                    # è´¨æ£€ç¯èŠ‚
                    if validator:
                        content = response.choices[0].message.content
                        if not validator(content):
                            raise ValueError(f"å†…å®¹è´¨æ£€æœªé€šè¿‡: {content[:50]}...")

                    return response

                except Exception as e:
                    llm_logger.log_failure(provider_id, str(e))
                    last_error = e
                    # ç»§ç»­å½“å‰ Provider çš„ä¸‹ä¸€æ¬¡é‡è¯•
                    continue
            
            # å½“å‰ Provider å·²ç”¨å°½æ‰€æœ‰é‡è¯•æ¬¡æ•°ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
            logger.warning(f"âš ï¸ Provider {provider_id} å·²ç”¨å°½ {max_retries} æ¬¡é‡è¯•ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª")

        # æ‰€æœ‰ Provider å‡å·²å°è¯•å®Œæ¯•
        llm_logger.log_exhausted()
        raise last_error or ValueError("æ‰€æœ‰ LLM é€šé“å‡ä¸å¯ç”¨")

    async def chat_stream(self, pool_name: str, messages: list,
                          temperature: float = 0.7, on_chunk=None):
        """
        æµå¼è°ƒç”¨ LLMï¼Œæ”¯æŒ:
          - é¡ºåºæ•…éšœè½¬ç§» (æŒ‰ priority é¡ºåº)
          - å•ç‚¹ç«­å°½é‡è¯• (æ¯ä¸ª Provider é‡è¯• N æ¬¡åå†åˆ‡æ¢)
          - å®æ—¶è¿”å›ç”Ÿæˆå†…å®¹
        
        Args:
            pool_name: æ± å­åç§° (metadata/analysis)
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            on_chunk: å¯é€‰çš„å›è°ƒå‡½æ•°ï¼Œæ¯æ”¶åˆ°ä¸€ä¸ª chunk æ—¶è°ƒç”¨
        
        Yields:
            str: æ¯ä¸ªç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        
        Returns:
            str: å®Œæ•´çš„ç”Ÿæˆå†…å®¹
        """
        target_pool = self.pools.get(pool_name, [])
        if not target_pool:
            raise ValueError(f"âŒ æ± å­ {pool_name} ä¸ºç©ºï¼Œè¯·åœ¨ç®¡ç†é¢æ¿é…ç½® LLM æä¾›å•†")

        max_retries = self._get_max_retries()
        last_error = None

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºéå†æ¯ä¸ª Provider
        for node in target_pool:
            provider_id = node['id']
            
            # å¯¹å½“å‰ Provider è¿›è¡Œæœ€å¤š max_retries æ¬¡å°è¯•
            for attempt in range(max_retries):
                try:
                    if attempt == 0:
                        llm_logger.log_request(pool_name, provider_id, node['api_type'], node.get('priority', 1))
                    else:
                        llm_logger.log_retry(attempt, max_retries, provider_id, str(last_error))

                    # ç›®å‰åªæœ‰ OpenAI å…¼å®¹ API æ”¯æŒæµå¼
                    if node["api_type"] not in ["openai"]:
                        # å¯¹äºä¸æ”¯æŒæµå¼çš„ APIï¼Œå›é€€åˆ°æ™®é€šè°ƒç”¨
                        logger.info(f"âš ï¸ {node['api_type']} ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨æ™®é€šè°ƒç”¨")
                        if node["api_type"] == "gemini":
                            response = await node["client"].create_chat_completion(
                                model=node["model"],
                                messages=messages,
                                temperature=temperature
                            )
                        elif node["api_type"] == "anthropic":
                            response = await node["client"].create_chat_completion(
                                model=node["model"],
                                messages=messages,
                                temperature=temperature
                            )
                        content = response.choices[0].message.content
                        if on_chunk:
                            on_chunk(content)
                        return content
                    
                    # OpenAI æµå¼è°ƒç”¨
                    full_content = ""
                    stream = await node["client"].chat.completions.create(
                        model=node["model"],
                        messages=messages,
                        temperature=temperature,
                        stream=True
                    )
                    
                    async for chunk in stream:
                        if chunk.choices and chunk.choices[0].delta.content:
                            content_piece = chunk.choices[0].delta.content
                            full_content += content_piece
                            if on_chunk:
                                on_chunk(content_piece)
                    
                    logger.info(f"âœ… æµå¼å“åº”å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_content)}")
                    return full_content

                except Exception as e:
                    llm_logger.log_failure(provider_id, str(e))
                    last_error = e
                    # ç»§ç»­å½“å‰ Provider çš„ä¸‹ä¸€æ¬¡é‡è¯•
                    continue
            
            # å½“å‰ Provider å·²ç”¨å°½æ‰€æœ‰é‡è¯•æ¬¡æ•°ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
            logger.warning(f"âš ï¸ Provider {provider_id} å·²ç”¨å°½ {max_retries} æ¬¡é‡è¯•ï¼Œåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª")

        # æ‰€æœ‰ Provider å‡å·²å°è¯•å®Œæ¯•
        llm_logger.log_exhausted()
        raise last_error or ValueError("æ‰€æœ‰ LLM é€šé“å‡ä¸å¯ç”¨")


# å…¨å±€å®ä¾‹
llm_manager = LLMManager()
